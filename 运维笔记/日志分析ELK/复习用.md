综合架构

filebeat->kafka->logstash->es->kibana

```powershell
Filebeat（轻采集） 
   → Kafka（缓冲 & 解耦） 
      → Logstash（解析、丰富、清洗）
         → Elasticsearch
```

https://www.elastic.co/cn/support/matrix  兼容信息

# 1.Elasticsearch

## 1.1配置及优化

裸机部署推荐物理内存的一半

```powershell
基于倒排索引的json文档数据库,近实时

单个进程可拥有的最大内存映射区域
echo "vm.max_map_count=262144" >> /etc/sysctl.conf
sysctl -p
```

配置文件

```powershell
/etc/elasticsearch/elasticsearch.yml
cluster.name: elk-cluster 单节点不需要
node.name: es-node1  当前节点名称
network.host: 10.0.0.169   监听ip
http.port: 9200      监听端口

discovery.seed_hosts: []
#discovery.type: single-node      单机
cluster.initial_master_nodes: [] 希望哪些节点可被选举为master，新加入节点不行，只有集群初始化的时候可以

path.logs:              包安装默认：/var/log/elasticsearch
path.data:              包安装默认：/var/lib/elasticsearch

以下都不写则都是混合节点
node.master: true      专职master节点，参与选举
node.data: true        专职data节点

以下两项关闭swap可解决
bootstrap.memory_lock: true    一开始就分配足够的内存，可能导致无法启动
[service]
LimitMEMLOCK=infinity    防止内存被交换到磁盘

LimitNOFILE=1000000 #修改最大打开的文件数，默认值为65535
LimitNPROC=65535    #修改打开最大的进程数，默认值为4096
```

如果是master和

内存优化建议

```powershell
在内存和数据量有一个建议的比例:对于一般日志类文件，1G 内存能存储48G~96GB数据
JVM 堆内存最大不要超过30GB
单个分片控制在30-50GB，太大查询会比较慢，索引恢复和更新时间越长；分片太小，会导致索引碎片化越严重，性能也会下降
```

做监控

```powershell
curl http://127.0.0.1:9200/_cluster/health?pretty=true
```

## 1.2概念

节点

```powershell
master负责集群管理，元数据管理，故障检测与恢复，健康检查，不参与实际数据存储和查询
Data 节点负责存储数据分片，执行数据操作，响应客户端请求
路由节点（Routing Node）就是协调节点（Coordinating Node），它主要负责接收请求、拆分任务、聚合结果，而不存储数据。

候选 master 保持集群元数据同步、监控 Active Master，并随时准备接管。只是它们不主动做“集群状态变更”的决策而已。
```

分片与副本

```powershell
Elasticsearch 不允许 主分片和它的副本放在同一节点，单机无副本，主分片读写，副本只读
请求->节点1（路由节点）->分片所在节点其中一个节点,也可能是节点1（非路由节点）->节点1->客户端
```

扩缩容

```powershell
扩容：配置文件cp后只需要改node.name，加两行node.master:false,node.data:true就行，其他不动

缩容：缩后加入的节点，只需要停止服务即可
```

## 1.3备份恢复

对象存储/共享存储

创建仓库

```powershell
PUT _snapshot/my_backup_repo
{
  "type": "fs",
  "settings": {
    "location": "/mnt/es_backups",  
    "compress": true
  }
}
```

创建快照

```powershell
PUT _snapshot/my_backup_repo/snapshot_2025_08_16?wait_for_completion=true
{
  "indices": "log-*",
  "ignore_unavailable": true,
  "include_global_state": true
}
```

恢复快照

```powershell
POST _snapshot/my_backup_repo/snapshot_2025_08_16/_restore
{
  "indices": "log-*",
  "rename_pattern": "log-(.*)",
  "rename_replacement": "restored-log-$1"
}
```



# 2logstash

```powershell
对数据进行聚合和处理，将数据发送到es

- 输入 Input: 用于日志收集,常见插件: Stdin、File、Kafka、Redis、Filebeat、Http
- 过滤 Filter: 日志过滤和转换,常用插件: grok、date、geoip、mutate、useragent
- 输出 Output: 将过滤转换过的日志输出, 常见插件: File,Stdout,Elasticsearch,MySQL,Redis,Kafka

Logstash 和 Filebeat 比较
- Logstash 功能更丰富,可以直接将非Json 格式的日志统一转换为Json格式,且支持多目标输出,比 filebeat有更为强大的过滤转换功能
- Logstash 资源消耗更多,不适合在每个日志主机上安装

setfacl控制对日志文件的权限
setfacl -m d:u:alice:rwX /project

rsyslog->logstash->es，rsyslog收集转发
```

## 2.1配置与优化

```powershell
/etc/logstash/logstash.yml
node.name: logstash-node01
pipeline.workers: 2
pipeline.batch.size: 1000 #批量从INPUT读取的消息个数，可以根据ES的性能做性能优化
pipeline.batch.delay: 5 #处理下一个事件前的最长等待时长，以毫秒ms为单位,可以根据ES的性能做性能优化
path.data: /var/lib/logstash #默认值
path.logs: /var/log/logstash #默认值

#内存优化
/etc/logstash/jvm.options
-Xms1g
-Xmx1g

Logstash默认以logstash用户运行,如果logstash需要收集本机的日志,可能会有权限问题,可以修改为root
```

```powershell
语法检查
logstash -f kafka_to_es.conf -t  没有-t就是单次执行
```

## 2.2filter插件

```powershell
常见的 Filter 插件:
利用 Grok 从非结构化数据中转化为结构数据
根据 IP 地址找出对应的地理坐标
利用 useragent 从请求中分析操作系统、设备类型
mutate插件主要是对字段进行类型转换，删除，替换，更新等操作
简化整体处理, 不受数据源、格式或架构的影响
logstash-plugin install logstash-filter-useragent

可以使用逻辑处理
```

怎么用？以前看百度或者看官方文档，**现在基本直接扔给AI处理**nginx

```powershell
log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                '$status $body_bytes_sent "$http_referer" '
                '"$http_user_agent" "$http_x_forwarded_for"';

#input {
#  beats {
#    port => 5044
#  }
#}
input {
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    sincedb_path => "/dev/null" # 每次重启都从头读取（测试用）
    
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2}"  # 匹配日志头
      negate => true                   # 不是头的行属于上一条日志
      what => "previous"               # 把非头行追加到上一条日志
    }
  }
}

filter {
  grok {
    match => { "message" => "%{IPORHOST:remote_addr} - %{DATA:remote_user} \[%{HTTPDATE:time_local}\] \"%{WORD:request_method} %{DATA:request_uri} HTTP/%{NUMBER:http_version}\" %{NUMBER:status} %{NUMBER:body_bytes_sent} \"%{DATA:http_referer}\" \"%{DATA:http_user_agent}\" \"%{DATA:http_x_forwarded_for}\"" }
  }

  # 把 time_local 转为 @timestamp
  date {
    match => [ "time_local", "dd/MMM/YYYY:HH:mm:ss Z" ]
    target => "@timestamp"
  }

  # 删除原始 message
  mutate {
    remove_field => ["message"]
  }
}

output {
  elasticsearch {
    hosts => ["http://es-cluster:9200"]
    index => "nginx-access-%{+YYYY.MM.dd}"
  }
}
```

apache

```powershell
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" 
  }

  geoip {
    source => "clientip"         # 从哪个字段取 IP
    target => "geoip"            # 存到哪个字段
    database => "/usr/share/GeoIP/GeoLite2-City.mmdb"  # 可选，默认用内置数据库
  }

  useragent {
    source => "agent"
    target => "ua"
  }
  mutate {
    remove_field => [ "[ua][build]" ]   # 删除不需要的字段
  }
```

## 2.3output插件

```powershell
output {
  elasticsearch {
    hosts => ["http://es-cluster:9200"]
    index => "logs-%{+YYYY.MM.dd}"
    user => "elastic"
    password => "changeme"
  }
}

还有file,http,stdout等等插件
```



# 3.kibana

```powershell
elasticsearch可视化工具
日志的话配个索引模式就完事了


可视化vusualize画图，在仪表板配置之后展示数据，dashboard往上也有模版
堆栈监测->使用内部收集设置->可以看到es和kibana主机情况
日志保留天数
nginx给账号密码

/etc/kibana/kibana.yml
```

配置文件

```powershell
/etc/kibana/kibana.yml
server.port: 5601   #监听端口,此为默认值

server.host: "0.0.0.0"  #修改此行的监听地址，默认为localhost，即：127.0.0.1:5601

#修改此行，指向ES任意服务器地址或多个节点地址实现容错，默认为localhost
elasticsearch.hosts:
["http://10.0.0.101:9200","http://10.0.0.102:9200","http://10.0.0.103:9200"]

i18n.locale: "zh-CN"    #修改此行，使用"zh-CN"显示中文界面，默认英文
```

## X-Pack

```powershell
核心模块：
| 模块 | 功能 |
|------|------|
| Security（安全） | 用户认证、角色权限控制、TLS 加密 |
| Monitoring（监控） | 集群/节点/索引监控仪表盘 |
| Alerting（告警） | 定义条件，触发邮件/Webhook 等告警 |
| Reporting（报表） | 报表生成与导出 PDF/CSV |
| Machine Learning | 异常检测、预测分析（需商业版许可） |
```

```powershell
# kibana.yml
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true  # 可选，开启节点间加密
xpack.monitoring.enabled: true
xpack.monitoring.kibana.collection.enabled: true

管理 → 安全 → 用户 & 角色→ 能看哪些 index、dashboard

bin/elasticsearch-setup-passwords interactive
默认内置用户：elastic, kibana, logstash_system 等。

告警（Alerting）
Kibana → Stack Management → Rules & Connectors
可以创建规则：
触发条件：比如文档数量 > 1000，CPU 使用 > 80%
动作：邮件通知、Slack/Webhook、日志写入等
```

## 基于日志关键字的告警

配置钉钉 Webhook Connector

```powershell
1.Kibana → Stack Management → Rules and Connectors → Connectors
2.创建 Webhook 类型 Connector
URL：钉钉群自定义机器人 Webhook
Method：POST
Headers / Body：按钉钉机器人要求填
```

创建日志关键字告警规则

```powershell
1.Kibana → Stack Management → Rules
2.点击 Create Rule
3.选择 Elasticsearch query 或 Index threshold
4.配置：
索引：日志所在 ES 索引（filebeat-*）
查询条件：
{
  "query": {
    "match_phrase": {
      "message": "update database error"
    }
  }
}
触发条件：
文档数量 > 0（即有匹配日志就触发）
时间窗口：最近 1 分钟

5.Actions → 选择前面创建的 Webhook Connector
6.保存规则

钉钉机器人：
Webhook 限制 20 条/分钟，告警量大时要注意
可以配合 聚合告警 或 抑制重复触发
发送大小有上限，不过巧了，es这边接收单条日志数据也可以调整上限，默认很大100M
```



# 4.kafka

```powershell
对于大流量日志进行排队处理
```

# 5.filebeat

```powershell
日志收集
/etc/filebeat/filebeat.yml
记录每个日志文件的读取位置（offset）
```

## 配置

### nginx日志

```powershell
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/nginx/access.log
    json.keys_under_root: true     # JSON 字段直接提升到根
    json.add_error_key: true       # 如果解析失败，记录 error.message
    json.overwrite_keys: true      # 覆盖同名字段
    fields:
      log_type: nginx_json
    fields_under_root: true

output.elasticsearch:
  hosts: ["http://es-node1:9200"]
  index: "nginx-json-%{+yyyy.MM.dd}"
```

更简单的，都不用后续做处理了

```powershell
filebeat.modules:
  - module: nginx
    access:
      enabled: true
      var.paths: ["/var/log/nginx/access.log"]
    error:
      enabled: true
      var.paths: ["/var/log/nginx/error.log"]

output.elasticsearch:
  hosts: ["http://es-cluster:9200"]

```

### java微服务日志

```powershell
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/myapp/*.log
    #multiline.* 解决 Java 堆栈多行日志 问题，把异常堆栈合并成一个事件。
    multiline.pattern: '^\d{4}-\d{2}-\d{2} '
    multiline.negate: true
    multiline.match: after
    processors:
      - dissect:
          tokenizer: "%{ts} %{+ts} %{level} %{rest}"
          field: "message"
          target_prefix: ""
      - drop_event:
          when:
            not:
              regexp:
                level: "INFO|ERROR|WARN"
    fields:
      service: myapp
    fields_under_root: true

output.elasticsearch:
  hosts: ["http://es-node1:9200"]
  indices:
    - index: "%{[service]}-info-%{+yyyy.MM.dd}"
      when.equals:
        level: "INFO"
    - index: "%{[service]}-error-%{+yyyy.MM.dd}"
      when.equals:
        level: "ERROR"
    - index: "%{[service]}-warn-%{+yyyy.MM.dd}"
      when.equals:
        level: "WARN"

```

```powershell
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/myapp/info-*.log
    fields:
      log_level: info
    fields_under_root: true
    json.keys_under_root: true     # 如果是 JSON 格式
    json.add_error_key: true
    index: "app-info-%{+yyyy.MM.dd}"  # 直接写入 app-info-2025.08.14 这种索引

  - type: log
    enabled: true
    paths:
      - /var/log/myapp/error-*.log
    fields:
      log_level: error
    fields_under_root: true
    json.keys_under_root: true
    json.add_error_key: true
    index: "app-error-%{+yyyy.MM.dd}"

output.elasticsearch:
  hosts: ["http://es-cluster:9200"]
  username: "elastic"
  password: "你的密码"
  # 这里 index 会被 inputs 里的 index 覆盖

```

### 传到kafka

```powershell
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/myapp/info-*.log
    fields:
      log_level: info
    fields_under_root: true
    json.keys_under_root: true
    json.add_error_key: true

  - type: log
    enabled: true
    paths:
      - /var/log/myapp/error-*.log
    fields:
      log_level: error
    fields_under_root: true
    json.keys_under_root: true
    json.add_error_key: true

output.kafka:
  hosts: ["kafka01:9092", "kafka02:9092", "kafka03:9092"]
  topic: "app-logs"                     # Kafka 统一一个 topic
  codec.json:
    pretty: false
  required_acks: -1        # 等同于 acks=all，1的话只保证leader接收，0不管
  compression: gzip
  max_message_bytes: 1000000
  timeout: 30s
  partition.round_robin:
    reachable_only: true   # 只发到可达 broker
```

logstash

```powershell
input {
  kafka {
    bootstrap_servers => "kafka01:9092,kafka02:9092,kafka03:9092"
    topics => ["app-logs"]
    codec => "json"
  }
}

filter {
  # 这里可以做额外字段处理
}

output {
  if [log_level] == "info" {
    elasticsearch {
      hosts => ["http://es-cluster:9200"]
      index => "app-info-%{+YYYY.MM.dd}"
    }
  } else if [log_level] == "error" {
    elasticsearch {
      hosts => ["http://es-cluster:9200"]
      index => "app-error-%{+YYYY.MM.dd}"
    }
  }
}

```

### 传到logstash

```powershell
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/myapp/info-*.log
    fields:
      log_level: info
    fields_under_root: true
    json.keys_under_root: true
    json.add_error_key: true

  - type: log
    enabled: true
    paths:
      - /var/log/myapp/error-*.log
    fields:
      log_level: error
    fields_under_root: true
    json.keys_under_root: true
    json.add_error_key: true

output.logstash:
  hosts: ["logstash01:5044", "logstash02:5044"]  # 多台 Logstash 可做 HA

```

logstash

```powershell
input {
  beats {
    port => 5044
  }
}

filter {
  # 可以在这里做额外字段处理，比如解析时间、过滤无用字段等
}

output {
  if [log_level] == "info" {
    elasticsearch {
      hosts => ["http://es-cluster:9200"]
      index => "app-info-%{+YYYY.MM.dd}"
    }
  } else if [log_level] == "error" {
    elasticsearch {
      hosts => ["http://es-cluster:9200"]
      index => "app-error-%{+YYYY.MM.dd}"
    }
  }
}

```

